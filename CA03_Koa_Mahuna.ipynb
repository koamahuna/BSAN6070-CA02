{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA03_Koa_Mahuna",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_WBn_D85npi",
        "outputId": "64f0dc6c-9304-433b-e72b-6767a3c8189a"
      },
      "source": [
        "\"\"\"naive_bayes.ipynb\r\n",
        "Automatically generated by Colaboratory.\r\n",
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "from collections import Counter\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive/')\r\n",
        "#imported necessary packages and mounted the google drive so the files would be accessible"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jpy2L525rL6"
      },
      "source": [
        "def create_Dict(root_dir):\r\n",
        "  all_words = []\r\n",
        "  messages = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\r\n",
        "  for item in messages:\r\n",
        "    with open(item) as m:\r\n",
        "      for line in m:\r\n",
        "        words = line.split()\r\n",
        "        all_words += words\r\n",
        "  dictionary = Counter(all_words)\r\n",
        "  list_to_remove = list(dictionary)\r\n",
        "\r\n",
        "  for word in list_to_remove:\r\n",
        "    if word.isalpha() == False:\r\n",
        "      del dictionary[item]\r\n",
        "    elif len(word) == 1:\r\n",
        "      del dictionary[word]\r\n",
        "  dictionary = dictionary.most_common(3000)\r\n",
        "  return dictionary\r\n",
        "\r\n",
        "#This function creates a dictionary that will contain the 3000 most common words in the messages. It begins by creating an empty list. This will house all words from all messages.\r\n",
        "#In order to populate that list we will first use a list comprehension that creates a list of the individual messages. Next we will iterate over each message and then split each message into \r\n",
        "#its subsequent pieces. We then save the individual pieces into a list. \r\n",
        "\r\n",
        "#In the next section of code, we will begin to clean the master list. We first begin by deleting the non-alphanumeric values. Secondly, if the item is only 1 letter long we will delete it.\r\n",
        "#Finally, we save the 3000 most frequently used words into our final dict.\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D16E8pA5x3_"
      },
      "source": [
        "def features(mail_dir):\r\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\r\n",
        "  features_matrix = np.zeros((len(files),3000))\r\n",
        "  train_labels = np.zeros(len(files))\r\n",
        "  count = 1;\r\n",
        "  docID = 0;\r\n",
        "  for fil in files:\r\n",
        "    with open(fil) as fi:\r\n",
        "      for i, line in enumerate(fi):\r\n",
        "        if i ==2:\r\n",
        "          words = line.split()\r\n",
        "          for word in words:\r\n",
        "            wordID = 0\r\n",
        "            for i, d in enumerate(dictionary):\r\n",
        "              if d[0] == word:\r\n",
        "                wordID = i\r\n",
        "                features_matrix[docID,wordID] = words.count(word)\r\n",
        "      train_labels[docID] = 0;\r\n",
        "      filepathTokens = fil.split('/')\r\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\r\n",
        "      if lastToken.startswith(\"spmsg\"):\r\n",
        "        train_labels[docID] = 1;\r\n",
        "        count = count + 1\r\n",
        "      docID = docID + 1\r\n",
        "  return features_matrix, train_labels\r\n",
        "\r\n",
        "\r\n",
        "#In this section we are creating a features matrix. We begin our function by creating nump arrays sized to the number of values we have. Again, we need to populate our\r\n",
        "#arrays so we will begin to gather the data from our directories. Now, we will open each file and check if the item is longer than one word. If it is then we split it and enumerate\r\n",
        "#these items sequentially. We then populate these arrays and index them by enumerating each item and setting that equal to the docID or wordID."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "whq6IvEsT5KK",
        "outputId": "70c62a69-5c2e-4db3-bbfd-301b447ac093"
      },
      "source": [
        "#Here we set our directories equal to the variables that we will be using in our model. Next we run our previously defined function on the first directory in order to prepare it for analysis.\r\n",
        "#Next, we will be setting our features matrix and labels equal to the output of our second function. We are taking these steps in order to prepare our data to be analyzed in the model.\r\n",
        "#We need to complete these preliminary steps to gather all of the necessary information to run our test. Now we can slect the model we want to use. In this case we are using the Gaussian \r\n",
        "#Model because we want to do classification of continuous variables. Now we can fit our model based on the data we gathered earlier. Finally, we can have our model make predictions.\r\n",
        "#We will be printing our accuracy score to see if the value is acceptable.\r\n",
        "\r\n",
        "\r\n",
        "TRAIN_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML Algorithms/Naive_Bayes/train-mails'\r\n",
        "TEST_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML Algorithms/Naive_Bayes/test-mails'\r\n",
        "\r\n",
        "dictionary = create_Dict(TRAIN_DIR)\r\n",
        "\r\n",
        "print(\"Preparing data for analysis\")\r\n",
        "features_matrix, labels = features(TRAIN_DIR)\r\n",
        "test_features_matrix, test_labels = features(TEST_DIR)\r\n",
        "\r\n",
        "#Selecting the type of naive bayes we want\r\n",
        "model = GaussianNB()\r\n",
        "\r\n",
        "print(\"Fitting the data to a Gaussian Naive Bayes\")\r\n",
        "model.fit(features_matrix, labels)\r\n",
        "\r\n",
        "print(\"Predicting data labels with trained model\")\r\n",
        "predicted_labels = model.predict(test_features_matrix)\r\n",
        "\r\n",
        "print(\"Predictions completed. The accuracy score of the model is:\")\r\n",
        "print(accuracy_score(test_labels, predicted_labels))\r\n",
        "\r\n",
        "\"\"\"======================= END OF PROGRAM =========================\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing data for analysis\n",
            "Fitting the data to a Gaussian Naive Bayes\n",
            "Predicting data labels with trained model\n",
            "Predictions completed. The accuracy score of the model is:\n",
            "0.9615384615384616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'======================= END OF PROGRAM ========================='"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jc1duc6l-fH"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}